$ CUDA_VISIBLE_DEVICES=0 python main_nsm.py --model_name gnn --data_folder /home/mist/cloud/Numerical/CWQ/ --checkpoint_dir /home/mist/cloud/Numerical/checkpoint/CWQ_num/ --entity2id entities_expanded.txt --batch_size 40 --test_batch_size 40 --num_step 4 --entity_dim 50 --word_dim 300 --node_dim 50 --eval_every 1 --experiment_name CWQ_gnn_num_50epoch --eps 0.95 --num_epoch 50 --use_self_loop --lr 1e-4 --q_type seq --word_emb_file word_emb_300d.npy --reason_kb --encode_type --loss_type kl --use_num --use_nsm_num --relation_embedding_file /home/mist/cloud/Numerical/CWQ/cwq_rel_embedding.npy --load_num /home/mist/cloud/Numerical/checkpoint/CWQ_num/CWQ_num_model.pth --load_experiment CWQ_nsm-h1.ckpt >train_num.log
2022-11-24 12:07:19,474 - root - INFO - PARAMETER----------
2022-11-24 12:07:19,474 - root - INFO - BATCH_SIZE=40
2022-11-24 12:07:19,474 - root - INFO - CACHE_DIR=/home/mist/numKBQA/cached_file/RoBERTa
2022-11-24 12:07:19,475 - root - INFO - CHAR2ID=chars.txt
2022-11-24 12:07:19,475 - root - INFO - CHECKPOINT_DIR=/home/mist/cloud/Numerical/checkpoint/CWQ_num/
2022-11-24 12:07:19,475 - root - INFO - DATA_FOLDER=/home/mist/cloud/Numerical/CWQ/
2022-11-24 12:07:19,475 - root - INFO - DECAY_RATE=0.0
2022-11-24 12:07:19,476 - root - INFO - ENCODE_TYPE=True
2022-11-24 12:07:19,476 - root - INFO - ENCODER_TYPE=lstm
2022-11-24 12:07:19,476 - root - INFO - ENTITY2ID=entities_expanded.txt
2022-11-24 12:07:19,476 - root - INFO - ENTITY_DIM=50
2022-11-24 12:07:19,476 - root - INFO - ENTITY_EMB_FILE=None
2022-11-24 12:07:19,477 - root - INFO - ENTITY_KGE_FILE=None
2022-11-24 12:07:19,477 - root - INFO - ENTROPY_WEIGHT=0.0
2022-11-24 12:07:19,477 - root - INFO - EPS=0.95
2022-11-24 12:07:19,477 - root - INFO - EVAL_EVERY=1
2022-11-24 12:07:19,477 - root - INFO - EXPERIMENT_NAME=CWQ_gnn_num_50epoch
2022-11-24 12:07:19,478 - root - INFO - FACT_DROP=0
2022-11-24 12:07:19,478 - root - INFO - FACT_SCALE=3
2022-11-24 12:07:19,478 - root - INFO - FILTER_LABEL=False
2022-11-24 12:07:19,478 - root - INFO - FILTER_SUB=False
2022-11-24 12:07:19,479 - root - INFO - GRADIENT_CLIP=1.0
2022-11-24 12:07:19,479 - root - INFO - IS_EVAL=False
2022-11-24 12:07:19,479 - root - INFO - KG_DIM=100
2022-11-24 12:07:19,479 - root - INFO - KGE_DIM=100
2022-11-24 12:07:19,479 - root - INFO - LABEL_F1=0.5
2022-11-24 12:07:19,480 - root - INFO - LABEL_FILE=None
2022-11-24 12:07:19,480 - root - INFO - LABEL_SMOOTH=0.1
2022-11-24 12:07:19,480 - root - INFO - LAMBDA_AUTO=0.01
2022-11-24 12:07:19,480 - root - INFO - LAMBDA_LABEL=0.1
2022-11-24 12:07:19,481 - root - INFO - LINEAR_DROPOUT=0.2
2022-11-24 12:07:19,481 - root - INFO - LOAD_CKPT_FILE=None
2022-11-24 12:07:19,481 - root - INFO - LOAD_EXPERIMENT=CWQ_nsm-h1.ckpt
2022-11-24 12:07:19,481 - root - INFO - LOAD_NUM=/home/mist/cloud/Numerical/checkpoint/CWQ_num/CWQ_num_model.pth
2022-11-24 12:07:19,482 - root - INFO - LOG_LEVEL=info
2022-11-24 12:07:19,482 - root - INFO - LOSS_TYPE=kl
2022-11-24 12:07:19,482 - root - INFO - LR=0.0001
2022-11-24 12:07:19,482 - root - INFO - LR_SCHEDULE=False
2022-11-24 12:07:19,483 - root - INFO - LSTM_DROPOUT=0.3
2022-11-24 12:07:19,483 - root - INFO - MODE=teacher
2022-11-24 12:07:19,483 - root - INFO - MODEL_NAME=gnn
2022-11-24 12:07:19,483 - root - INFO - NAME=webqsp
2022-11-24 12:07:19,484 - root - INFO - NODE_DIM=50
2022-11-24 12:07:19,484 - root - INFO - NUM_DROPOUT=0.1
2022-11-24 12:07:19,484 - root - INFO - NUM_EPOCH=50
2022-11-24 12:07:19,484 - root - INFO - NUM_LAYER=1
2022-11-24 12:07:19,485 - root - INFO - NUM_STEP=4
2022-11-24 12:07:19,485 - root - INFO - NUM_VEC_FUNC=concat
2022-11-24 12:07:19,485 - root - INFO - PRETRAINED_ENTITY_KGE_FILE=entity_emb_100d.npy
2022-11-24 12:07:19,485 - root - INFO - Q_TYPE=seq
2022-11-24 12:07:19,486 - root - INFO - REASON_KB=True
2022-11-24 12:07:19,486 - root - INFO - REL_WORD_IDS=rel_word_idx.npy
2022-11-24 12:07:19,486 - root - INFO - RELATION2ID=relations.txt
2022-11-24 12:07:19,486 - root - INFO - RELATION_EMB_FILE=None
2022-11-24 12:07:19,486 - root - INFO - RELATION_EMBEDDING_FILE=/home/mist/cloud/Numerical/CWQ/cwq_rel_embedding.npy
2022-11-24 12:07:19,487 - root - INFO - RELATION_KGE_FILE=None
2022-11-24 12:07:19,487 - root - INFO - SEED=19960626
2022-11-24 12:07:19,487 - root - INFO - SHARE_EMBEDDING=False
2022-11-24 12:07:19,487 - root - INFO - SHARE_ENCODER=False
2022-11-24 12:07:19,488 - root - INFO - SHARE_INSTRUCTION=False
2022-11-24 12:07:19,488 - root - INFO - TEST_BATCH_SIZE=40
2022-11-24 12:07:19,488 - root - INFO - TRAIN_KL=False
2022-11-24 12:07:19,488 - root - INFO - TREE_SOFT=False
2022-11-24 12:07:19,488 - root - INFO - USE_CUDA=True
2022-11-24 12:07:19,489 - root - INFO - USE_INVERSE_RELATION=False
2022-11-24 12:07:19,489 - root - INFO - USE_LABEL=False
2022-11-24 12:07:19,489 - root - INFO - USE_NSM_NUM=True
2022-11-24 12:07:19,489 - root - INFO - USE_NUM=True
2022-11-24 12:07:19,490 - root - INFO - USE_SELF_LOOP=True
2022-11-24 12:07:19,490 - root - INFO - WORD2ID=vocab_new.txt
2022-11-24 12:07:19,490 - root - INFO - WORD_DIM=300
2022-11-24 12:07:19,490 - root - INFO - WORD_EMB_FILE=word_emb_300d.npy
2022-11-24 12:07:19,490 - root - INFO - -------------------
27639it [02:00, 228.56it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 27631/27631 [00:07<00:00, 3558.97it/s]
loading file vocab.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json
loading file merges.txt from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.24.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 27631/27631 [00:00<00:00, 32742.51it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 27631/27631 [02:27<00:00, 187.06it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 27631/27631 [00:01<00:00, 15797.38it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 27631/27631 [00:11<00:00, 2459.47it/s]
3519it [00:09, 373.90it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3519/3519 [00:01<00:00, 3419.05it/s]
loading file vocab.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json
loading file merges.txt from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.24.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3519/3519 [00:00<00:00, 31799.40it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3519/3519 [00:19<00:00, 179.90it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3519/3519 [00:00<00:00, 15911.44it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3519/3519 [00:01<00:00, 2532.37it/s]
3531it [00:21, 161.10it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3531/3531 [00:01<00:00, 3450.00it/s]
loading file vocab.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json
loading file merges.txt from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.24.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3531/3531 [00:00<00:00, 32099.47it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3531/3531 [00:20<00:00, 176.26it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3531/3531 [00:00<00:00, 15182.34it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3531/3531 [00:01<00:00, 2540.81it/s]
2022-11-24 12:13:38,262 - root - INFO - Building Agent.
loading configuration file config.json from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.24.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file pytorch_model.bin from cache at /home/mist/numKBQA/cached_file/RoBERTa/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.
2022-11-24 12:13:44,717 - root - INFO - Architecture: NsmAgent(
  (model): GNNModel(
    (relation_embedding): Embedding(6650, 200)
    (word_embedding): Embedding(20050, 300, padding_idx=20049)
    (entity_linear): Linear(in_features=100, out_features=50, bias=True)
    (relation_linear): Linear(in_features=200, out_features=50, bias=True)
    (lstm_drop): Dropout(p=0.3, inplace=False)
    (linear_drop): Dropout(p=0.2, inplace=False)
    (type_layer): TypeLayer(
      (linear_drop): Dropout(p=0.2, inplace=False)
      (kb_self_linear): Linear(in_features=50, out_features=50, bias=True)
    )
    (kld_loss): KLDivLoss()
    (bce_loss_logits): BCEWithLogitsLoss()
    (mse_loss): MSELoss()
    (reasoning): NumNSMReasoning(
      (lstm_drop): Dropout(p=0.3, inplace=False)
      (linear_drop): Dropout(p=0.2, inplace=False)
      (softmax_d1): Softmax(dim=1)
      (score_func): Linear(in_features=50, out_features=1, bias=True)
      (rel_linear0): Linear(in_features=50, out_features=50, bias=True)
      (e2e_linear0): Linear(in_features=100, out_features=50, bias=True)
      (rel_linear1): Linear(in_features=50, out_features=50, bias=True)
      (e2e_linear1): Linear(in_features=100, out_features=50, bias=True)
      (rel_linear2): Linear(in_features=50, out_features=50, bias=True)
      (e2e_linear2): Linear(in_features=100, out_features=50, bias=True)
      (rel_linear3): Linear(in_features=50, out_features=50, bias=True)
      (e2e_linear3): Linear(in_features=100, out_features=50, bias=True)
      (roberta_model): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(50265, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): RobertaPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (hidden2ent): Linear(in_features=768, out_features=50, bias=True)
      (number_embedding): Embedding(2566910, 50, padding_idx=2566908)
      (number_relation_embedding): Embedding(6716, 50)
      (_gcn_input_proj): Linear(in_features=100, out_features=50, bias=True)
      (_gcn): GCN(
        (_node_weight_fc): Linear(in_features=50, out_features=1, bias=True)
        (_self_node_fc): Linear(in_features=50, out_features=50, bias=True)
        (_dd_node_fc_left): Linear(in_features=50, out_features=50, bias=False)
      )
      (_proj_ln): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (_gcn_enc): ResidualGRU(
        (enc_layer): GRU(50, 25, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
        (enc_ln): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      )
      (sep_embedding): Embedding(1, 50)
      (transformer_encoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
            )
            (linear1): Linear(in_features=50, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=50, bias=True)
            (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)
            )
            (linear1): Linear(in_features=50, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=50, bias=True)
            (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (reduce_emb_linear): Linear(in_features=150, out_features=50, bias=True)
      (kb_head_linear): Linear(in_features=50, out_features=50, bias=True)
      (kb_tail_linear): Linear(in_features=50, out_features=50, bias=True)
      (kb_self_linear): Linear(in_features=50, out_features=50, bias=True)
      (rel_linear_num): Linear(in_features=50, out_features=50, bias=True)
      (e2e_linear_num): Linear(in_features=100, out_features=50, bias=True)
      (relu): ReLU()
      (score_func_num): Linear(in_features=100, out_features=1, bias=True)
    )
    (instruction): LSTMInstruction(
      (lstm_drop): Dropout(p=0.3, inplace=False)
      (linear_drop): Dropout(p=0.2, inplace=False)
      (word_embedding): Embedding(20050, 300, padding_idx=20049)
      (node_encoder): LSTM(300, 50, batch_first=True)
      (cq_linear): Linear(in_features=100, out_features=50, bias=True)
      (ca_linear): Linear(in_features=50, out_features=1, bias=True)
      (question_linear0): Linear(in_features=50, out_features=50, bias=True)
      (question_linear1): Linear(in_features=50, out_features=50, bias=True)
      (question_linear2): Linear(in_features=50, out_features=50, bias=True)
      (question_linear3): Linear(in_features=50, out_features=50, bias=True)
    )
  )
)
2022-11-24 12:13:44,720 - root - INFO - Agent params: 261335182.0
2022-11-24 12:13:56,467 - root - INFO - Load param of node_embedding.weight, roberta_model.embeddings.position_ids, roberta_model.embeddings.word_embeddings.weight, roberta_model.embeddings.position_embeddings.weight, roberta_model.embeddings.token_type_embeddings.weight, roberta_model.embeddings.LayerNorm.weight, roberta_model.embeddings.LayerNorm.bias, roberta_model.encoder.layer.0.attention.self.query.weight, roberta_model.encoder.layer.0.attention.self.query.bias, roberta_model.encoder.layer.0.attention.self.key.weight, roberta_model.encoder.layer.0.attention.self.key.bias, roberta_model.encoder.layer.0.attention.self.value.weight, roberta_model.encoder.layer.0.attention.self.value.bias, roberta_model.encoder.layer.0.attention.output.dense.weight, roberta_model.encoder.layer.0.attention.output.dense.bias, roberta_model.encoder.layer.0.attention.output.LayerNorm.weight, roberta_model.encoder.layer.0.attention.output.LayerNorm.bias, roberta_model.encoder.layer.0.intermediate.dense.weight, roberta_model.encoder.layer.0.intermediate.dense.bias, roberta_model.encoder.layer.0.output.dense.weight, roberta_model.encoder.layer.0.output.dense.bias, roberta_model.encoder.layer.0.output.LayerNorm.weight, roberta_model.encoder.layer.0.output.LayerNorm.bias, roberta_model.encoder.layer.1.attention.self.query.weight, roberta_model.encoder.layer.1.attention.self.query.bias, roberta_model.encoder.layer.1.attention.self.key.weight, roberta_model.encoder.layer.1.attention.self.key.bias, roberta_model.encoder.layer.1.attention.self.value.weight, roberta_model.encoder.layer.1.attention.self.value.bias, roberta_model.encoder.layer.1.attention.output.dense.weight, roberta_model.encoder.layer.1.attention.output.dense.bias, roberta_model.encoder.layer.1.attention.output.LayerNorm.weight, roberta_model.encoder.layer.1.attention.output.LayerNorm.bias, roberta_model.encoder.layer.1.intermediate.dense.weight, roberta_model.encoder.layer.1.intermediate.dense.bias, roberta_model.encoder.layer.1.output.dense.weight, roberta_model.encoder.layer.1.output.dense.bias, roberta_model.encoder.layer.1.output.LayerNorm.weight, roberta_model.encoder.layer.1.output.LayerNorm.bias, roberta_model.encoder.layer.2.attention.self.query.weight, roberta_model.encoder.layer.2.attention.self.query.bias, roberta_model.encoder.layer.2.attention.self.key.weight, roberta_model.encoder.layer.2.attention.self.key.bias, roberta_model.encoder.layer.2.attention.self.value.weight, roberta_model.encoder.layer.2.attention.self.value.bias, roberta_model.encoder.layer.2.attention.output.dense.weight, roberta_model.encoder.layer.2.attention.output.dense.bias, roberta_model.encoder.layer.2.attention.output.LayerNorm.weight, roberta_model.encoder.layer.2.attention.output.LayerNorm.bias, roberta_model.encoder.layer.2.intermediate.dense.weight, roberta_model.encoder.layer.2.intermediate.dense.bias, roberta_model.encoder.layer.2.output.dense.weight, roberta_model.encoder.layer.2.output.dense.bias, roberta_model.encoder.layer.2.output.LayerNorm.weight, roberta_model.encoder.layer.2.output.LayerNorm.bias, roberta_model.encoder.layer.3.attention.self.query.weight, roberta_model.encoder.layer.3.attention.self.query.bias, roberta_model.encoder.layer.3.attention.self.key.weight, roberta_model.encoder.layer.3.attention.self.key.bias, roberta_model.encoder.layer.3.attention.self.value.weight, roberta_model.encoder.layer.3.attention.self.value.bias, roberta_model.encoder.layer.3.attention.output.dense.weight, roberta_model.encoder.layer.3.attention.output.dense.bias, roberta_model.encoder.layer.3.attention.output.LayerNorm.weight, roberta_model.encoder.layer.3.attention.output.LayerNorm.bias, roberta_model.encoder.layer.3.intermediate.dense.weight, roberta_model.encoder.layer.3.intermediate.dense.bias, roberta_model.encoder.layer.3.output.dense.weight, roberta_model.encoder.layer.3.output.dense.bias, roberta_model.encoder.layer.3.output.LayerNorm.weight, roberta_model.encoder.layer.3.output.LayerNorm.bias, roberta_model.encoder.layer.4.attention.self.query.weight, roberta_model.encoder.layer.4.attention.self.query.bias, roberta_model.encoder.layer.4.attention.self.key.weight, roberta_model.encoder.layer.4.attention.self.key.bias, roberta_model.encoder.layer.4.attention.self.value.weight, roberta_model.encoder.layer.4.attention.self.value.bias, roberta_model.encoder.layer.4.attention.output.dense.weight, roberta_model.encoder.layer.4.attention.output.dense.bias, roberta_model.encoder.layer.4.attention.output.LayerNorm.weight, roberta_model.encoder.layer.4.attention.output.LayerNorm.bias, roberta_model.encoder.layer.4.intermediate.dense.weight, roberta_model.encoder.layer.4.intermediate.dense.bias, roberta_model.encoder.layer.4.output.dense.weight, roberta_model.encoder.layer.4.output.dense.bias, roberta_model.encoder.layer.4.output.LayerNorm.weight, roberta_model.encoder.layer.4.output.LayerNorm.bias, roberta_model.encoder.layer.5.attention.self.query.weight, roberta_model.encoder.layer.5.attention.self.query.bias, roberta_model.encoder.layer.5.attention.self.key.weight, roberta_model.encoder.layer.5.attention.self.key.bias, roberta_model.encoder.layer.5.attention.self.value.weight, roberta_model.encoder.layer.5.attention.self.value.bias, roberta_model.encoder.layer.5.attention.output.dense.weight, roberta_model.encoder.layer.5.attention.output.dense.bias, roberta_model.encoder.layer.5.attention.output.LayerNorm.weight, roberta_model.encoder.layer.5.attention.output.LayerNorm.bias, roberta_model.encoder.layer.5.intermediate.dense.weight, roberta_model.encoder.layer.5.intermediate.dense.bias, roberta_model.encoder.layer.5.output.dense.weight, roberta_model.encoder.layer.5.output.dense.bias, roberta_model.encoder.layer.5.output.LayerNorm.weight, roberta_model.encoder.layer.5.output.LayerNorm.bias, roberta_model.encoder.layer.6.attention.self.query.weight, roberta_model.encoder.layer.6.attention.self.query.bias, roberta_model.encoder.layer.6.attention.self.key.weight, roberta_model.encoder.layer.6.attention.self.key.bias, roberta_model.encoder.layer.6.attention.self.value.weight, roberta_model.encoder.layer.6.attention.self.value.bias, roberta_model.encoder.layer.6.attention.output.dense.weight, roberta_model.encoder.layer.6.attention.output.dense.bias, roberta_model.encoder.layer.6.attention.output.LayerNorm.weight, roberta_model.encoder.layer.6.attention.output.LayerNorm.bias, roberta_model.encoder.layer.6.intermediate.dense.weight, roberta_model.encoder.layer.6.intermediate.dense.bias, roberta_model.encoder.layer.6.output.dense.weight, roberta_model.encoder.layer.6.output.dense.bias, roberta_model.encoder.layer.6.output.LayerNorm.weight, roberta_model.encoder.layer.6.output.LayerNorm.bias, roberta_model.encoder.layer.7.attention.self.query.weight, roberta_model.encoder.layer.7.attention.self.query.bias, roberta_model.encoder.layer.7.attention.self.key.weight, roberta_model.encoder.layer.7.attention.self.key.bias, roberta_model.encoder.layer.7.attention.self.value.weight, roberta_model.encoder.layer.7.attention.self.value.bias, roberta_model.encoder.layer.7.attention.output.dense.weight, roberta_model.encoder.layer.7.attention.output.dense.bias, roberta_model.encoder.layer.7.attention.output.LayerNorm.weight, roberta_model.encoder.layer.7.attention.output.LayerNorm.bias, roberta_model.encoder.layer.7.intermediate.dense.weight, roberta_model.encoder.layer.7.intermediate.dense.bias, roberta_model.encoder.layer.7.output.dense.weight, roberta_model.encoder.layer.7.output.dense.bias, roberta_model.encoder.layer.7.output.LayerNorm.weight, roberta_model.encoder.layer.7.output.LayerNorm.bias, roberta_model.encoder.layer.8.attention.self.query.weight, roberta_model.encoder.layer.8.attention.self.query.bias, roberta_model.encoder.layer.8.attention.self.key.weight, roberta_model.encoder.layer.8.attention.self.key.bias, roberta_model.encoder.layer.8.attention.self.value.weight, roberta_model.encoder.layer.8.attention.self.value.bias, roberta_model.encoder.layer.8.attention.output.dense.weight, roberta_model.encoder.layer.8.attention.output.dense.bias, roberta_model.encoder.layer.8.attention.output.LayerNorm.weight, roberta_model.encoder.layer.8.attention.output.LayerNorm.bias, roberta_model.encoder.layer.8.intermediate.dense.weight, roberta_model.encoder.layer.8.intermediate.dense.bias, roberta_model.encoder.layer.8.output.dense.weight, roberta_model.encoder.layer.8.output.dense.bias, roberta_model.encoder.layer.8.output.LayerNorm.weight, roberta_model.encoder.layer.8.output.LayerNorm.bias, roberta_model.encoder.layer.9.attention.self.query.weight, roberta_model.encoder.layer.9.attention.self.query.bias, roberta_model.encoder.layer.9.attention.self.key.weight, roberta_model.encoder.layer.9.attention.self.key.bias, roberta_model.encoder.layer.9.attention.self.value.weight, roberta_model.encoder.layer.9.attention.self.value.bias, roberta_model.encoder.layer.9.attention.output.dense.weight, roberta_model.encoder.layer.9.attention.output.dense.bias, roberta_model.encoder.layer.9.attention.output.LayerNorm.weight, roberta_model.encoder.layer.9.attention.output.LayerNorm.bias, roberta_model.encoder.layer.9.intermediate.dense.weight, roberta_model.encoder.layer.9.intermediate.dense.bias, roberta_model.encoder.layer.9.output.dense.weight, roberta_model.encoder.layer.9.output.dense.bias, roberta_model.encoder.layer.9.output.LayerNorm.weight, roberta_model.encoder.layer.9.output.LayerNorm.bias, roberta_model.encoder.layer.10.attention.self.query.weight, roberta_model.encoder.layer.10.attention.self.query.bias, roberta_model.encoder.layer.10.attention.self.key.weight, roberta_model.encoder.layer.10.attention.self.key.bias, roberta_model.encoder.layer.10.attention.self.value.weight, roberta_model.encoder.layer.10.attention.self.value.bias, roberta_model.encoder.layer.10.attention.output.dense.weight, roberta_model.encoder.layer.10.attention.output.dense.bias, roberta_model.encoder.layer.10.attention.output.LayerNorm.weight, roberta_model.encoder.layer.10.attention.output.LayerNorm.bias, roberta_model.encoder.layer.10.intermediate.dense.weight, roberta_model.encoder.layer.10.intermediate.dense.bias, roberta_model.encoder.layer.10.output.dense.weight, roberta_model.encoder.layer.10.output.dense.bias, roberta_model.encoder.layer.10.output.LayerNorm.weight, roberta_model.encoder.layer.10.output.LayerNorm.bias, roberta_model.encoder.layer.11.attention.self.query.weight, roberta_model.encoder.layer.11.attention.self.query.bias, roberta_model.encoder.layer.11.attention.self.key.weight, roberta_model.encoder.layer.11.attention.self.key.bias, roberta_model.encoder.layer.11.attention.self.value.weight, roberta_model.encoder.layer.11.attention.self.value.bias, roberta_model.encoder.layer.11.attention.output.dense.weight, roberta_model.encoder.layer.11.attention.output.dense.bias, roberta_model.encoder.layer.11.attention.output.LayerNorm.weight, roberta_model.encoder.layer.11.attention.output.LayerNorm.bias, roberta_model.encoder.layer.11.intermediate.dense.weight, roberta_model.encoder.layer.11.intermediate.dense.bias, roberta_model.encoder.layer.11.output.dense.weight, roberta_model.encoder.layer.11.output.dense.bias, roberta_model.encoder.layer.11.output.LayerNorm.weight, roberta_model.encoder.layer.11.output.LayerNorm.bias, roberta_model.pooler.dense.weight, roberta_model.pooler.dense.bias, hidden2ent.weight, hidden2ent.bias, _gcn._node_weight_fc.weight, _gcn._node_weight_fc.bias, _gcn._self_node_fc.weight, _gcn._self_node_fc.bias, _gcn._dd_node_fc_left.weight, _proj_ln.weight, _proj_ln.bias, _gcn_enc.enc_layer.weight_ih_l0, _gcn_enc.enc_layer.weight_hh_l0, _gcn_enc.enc_layer.bias_ih_l0, _gcn_enc.enc_layer.bias_hh_l0, _gcn_enc.enc_layer.weight_ih_l0_reverse, _gcn_enc.enc_layer.weight_hh_l0_reverse, _gcn_enc.enc_layer.bias_ih_l0_reverse, _gcn_enc.enc_layer.bias_hh_l0_reverse, _gcn_enc.enc_layer.weight_ih_l1, _gcn_enc.enc_layer.weight_hh_l1, _gcn_enc.enc_layer.bias_ih_l1, _gcn_enc.enc_layer.bias_hh_l1, _gcn_enc.enc_layer.weight_ih_l1_reverse, _gcn_enc.enc_layer.weight_hh_l1_reverse, _gcn_enc.enc_layer.bias_ih_l1_reverse, _gcn_enc.enc_layer.bias_hh_l1_reverse, _gcn_enc.enc_ln.weight, _gcn_enc.enc_ln.bias, transformer_encoder.layers.0.self_attn.in_proj_weight, transformer_encoder.layers.0.self_attn.in_proj_bias, transformer_encoder.layers.0.self_attn.out_proj.weight, transformer_encoder.layers.0.self_attn.out_proj.bias, transformer_encoder.layers.0.linear1.weight, transformer_encoder.layers.0.linear1.bias, transformer_encoder.layers.0.linear2.weight, transformer_encoder.layers.0.linear2.bias, transformer_encoder.layers.0.norm1.weight, transformer_encoder.layers.0.norm1.bias, transformer_encoder.layers.0.norm2.weight, transformer_encoder.layers.0.norm2.bias, transformer_encoder.layers.1.self_attn.in_proj_weight, transformer_encoder.layers.1.self_attn.in_proj_bias, transformer_encoder.layers.1.self_attn.out_proj.weight, transformer_encoder.layers.1.self_attn.out_proj.bias, transformer_encoder.layers.1.linear1.weight, transformer_encoder.layers.1.linear1.bias, transformer_encoder.layers.1.linear2.weight, transformer_encoder.layers.1.linear2.bias, transformer_encoder.layers.1.norm1.weight, transformer_encoder.layers.1.norm1.bias, transformer_encoder.layers.1.norm2.weight, transformer_encoder.layers.1.norm2.bias, sep_embedding.weight, numq_reduce_linear.weight, numq_reduce_linear.bias, reduce_linear_2.weight, reduce_linear_2.bias, reduce_linear.weight, reduce_linear.bias from /home/mist/cloud/Numerical/checkpoint/CWQ_num/CWQ_num_model.pth.
2022-11-24 12:13:56,563 - root - INFO - Load param of relation_embedding.weight, word_embedding.weight, entity_linear.weight, entity_linear.bias, relation_linear.weight, relation_linear.bias, type_layer.kb_self_linear.weight, type_layer.kb_self_linear.bias, reasoning.score_func.weight, reasoning.score_func.bias, reasoning.rel_linear0.weight, reasoning.rel_linear0.bias, reasoning.e2e_linear0.weight, reasoning.e2e_linear0.bias, reasoning.rel_linear1.weight, reasoning.rel_linear1.bias, reasoning.e2e_linear1.weight, reasoning.e2e_linear1.bias, reasoning.rel_linear2.weight, reasoning.rel_linear2.bias, reasoning.e2e_linear2.weight, reasoning.e2e_linear2.bias, reasoning.rel_linear3.weight, reasoning.rel_linear3.bias, reasoning.e2e_linear3.weight, reasoning.e2e_linear3.bias, instruction.word_embedding.weight, instruction.node_encoder.weight_ih_l0, instruction.node_encoder.weight_hh_l0, instruction.node_encoder.bias_ih_l0, instruction.node_encoder.bias_hh_l0, instruction.cq_linear.weight, instruction.cq_linear.bias, instruction.ca_linear.weight, instruction.ca_linear.bias, instruction.question_linear0.weight, instruction.question_linear0.bias, instruction.question_linear1.weight, instruction.question_linear1.bias, instruction.question_linear2.weight, instruction.question_linear2.bias, instruction.question_linear3.weight, instruction.question_linear3.bias from /home/mist/cloud/Numerical/checkpoint/CWQ_num/CWQ_nsm-h1.ckpt.
  0%|                                                                                                                                                                       | 0/88 [00:00<?, ?it/s]/home/mist/numKBQA/NumReasoning/NSM/Modules/layer_nsm.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /home/mist/pytorch/torch/csrc/utils/tensor_new.cpp:210.)
  fact2head = torch.LongTensor([batch_heads, fact_ids]).to(self.device)
/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5279: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.
  warnings.warn("Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.")
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 88/88 [01:10<00:00,  1.24it/s]
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 89/89 [01:11<00:00,  1.24it/s]
2022-11-24 12:16:19,038 - root - INFO - initial TEST F1: 0.3767, H1: 0.4342
2022-11-24 12:16:19,038 - root - INFO - initial superlative acc: 0.0152
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 691/691 [09:33<00:00,  1.21it/s]
2022-11-24 12:25:52,220 - root - INFO - Epoch: 1, loss : 0.9274, time: 573.1810789108276
2022-11-24 12:25:52,226 - root - INFO - Training h1 : 0.5981, f1 : 0.4633
2022-11-24 12:25:52,244 - root - INFO - Training type hits: comparative type question,  733 cases among 1531 cases are right, accuracy: 0.4788, superlative type question,  478 cases among 1435 cases are right, accuracy: 0.3331, unknown type question,  15315 cases among 24665 cases are right, accuracy: 0.6209
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 88/88 [01:09<00:00,  1.27it/s]
2022-11-24 12:27:01,499 - root - INFO - EVAL F1: 0.4542, H1: 0.5109
2022-11-24 12:27:01,500 - root - INFO - comparative type question,  93 cases among 219 cases are right, accuracy: 0.4247, superlative type question,  48 cases among 191 cases are right, accuracy: 0.2513, unknown type question,  1657 cases among 3109 cases are right, accuracy: 0.5330
2022-11-24 12:34:35,839 - root - INFO - Best dev superlative acc: 0.251
100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 89/89 [01:11<00:00,  1.25it/s]
2022-11-24 12:35:47,024 - root - INFO - TEST F1: 0.4147, H1: 0.4670
2022-11-24 12:35:47,024 - root - INFO - comparative type question,  79 cases among 213 cases are right, accuracy: 0.3709, superlative type question,  49 cases among 197 cases are right, accuracy: 0.2487, unknown type question,  1521 cases among 3121 cases are right, accuracy: 0.4873
